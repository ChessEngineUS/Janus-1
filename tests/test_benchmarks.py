#!/usr/bin/env python3
"""
Benchmark Validation Test Suite
================================

Validates benchmark harness and performance measurement accuracy.
Ensures benchmarking infrastructure produces reliable results.

Author: Janus-1 Design Team
License: MIT
"""

import pytest
import time
import numpy as np
from src.simulator.janus_sim import JanusSim, SimulationConfig
from src.benchmarks.trace_generator import generate_llm_trace, generate_sequential_trace


class TestBenchmarkAccuracy:
    """Test benchmark measurement accuracy."""

    def test_cycle_count_accuracy(self):
        """Test cycle counting is accurate and deterministic."""
        config = SimulationConfig()
        sim = JanusSim(config)
        
        trace = generate_sequential_trace(start_addr=0x10000, num_accesses=100)
        
        sim.run(trace)
        metrics1 = sim.get_metrics()
        
        # Run again
        sim2 = JanusSim(config)
        sim2.run(trace)
        metrics2 = sim2.get_metrics()
        
        assert metrics1.total_cycles == metrics2.total_cycles, \
            "Cycle count should be deterministic"

    def test_latency_measurement_precision(self):
        """Test latency measurements have cycle-level precision."""
        sim = JanusSim()
        
        addr = 0x20000
        trace = [("READ", addr), ("READ", addr)]  # Miss then hit
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Latencies should be integer cycles
        assert all(isinstance(lat, (int, np.integer)) or lat.is_integer() 
                  for lat in metrics.read_latencies), \
            "Latencies should be integer cycles"

    def test_hit_rate_precision(self):
        """Test hit rate calculation precision."""
        sim = JanusSim()
        
        # Create trace with known hit pattern
        # 1 miss + 99 hits = 99% hit rate
        addr = 0x30000
        trace = [("READ", addr)] * 100
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        expected_rate = 99.0
        assert abs(metrics.hit_rate - expected_rate) < 0.01, \
            f"Hit rate should be {expected_rate}%, got {metrics.hit_rate:.2f}%"


class TestBenchmarkReproducibility:
    """Test benchmark reproducibility."""

    def test_deterministic_simulation(self):
        """Test simulation is fully deterministic."""
        trace = generate_llm_trace(context_length=256, hidden_dim=2048)
        
        results = []
        for _ in range(3):
            sim = JanusSim()
            sim.run(trace)
            metrics = sim.get_metrics()
            results.append((
                metrics.t1_hits,
                metrics.t1_misses,
                metrics.total_cycles,
                metrics.hit_rate
            ))
        
        # All runs should be identical
        assert len(set(results)) == 1, "Simulation should be deterministic"

    def test_trace_generator_reproducibility(self):
        """Test trace generation is reproducible."""
        # Using default parameters should produce same trace
        trace1 = generate_llm_trace(context_length=128, hidden_dim=1024)
        trace2 = generate_llm_trace(context_length=128, hidden_dim=1024)
        
        # If no randomness, should be identical
        # (or provide seed parameter for reproducibility)
        assert len(trace1) == len(trace2), "Traces should have same length"


class TestPerformanceRegression:
    """Test for performance regressions."""

    def test_baseline_llm_performance(self):
        """Test baseline LLM workload performance."""
        config = SimulationConfig(prefetch_look_ahead=16)
        sim = JanusSim(config)
        
        trace = generate_llm_trace(context_length=2048, hidden_dim=4096)
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Baseline targets
        assert metrics.hit_rate >= 99.5, \
            f"Baseline hit rate regression: {metrics.hit_rate:.2f}% < 99.5%"
        
        if metrics.hit_rate > 99.0:
            assert metrics.p99_latency <= 2.0, \
                f"Baseline P99 latency regression: {metrics.p99_latency:.1f} > 2.0 cycles"

    def test_sequential_workload_performance(self):
        """Test sequential workload performance baseline."""
        config = SimulationConfig(prefetch_look_ahead=16)
        sim = JanusSim(config)
        
        trace = generate_sequential_trace(start_addr=0x40000, num_accesses=5000)
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Sequential with prefetcher should achieve >98% hit rate
        assert metrics.hit_rate >= 98.0, \
            f"Sequential performance regression: {metrics.hit_rate:.2f}% < 98.0%"

    def test_simulation_runtime_performance(self):
        """Test simulation completes in reasonable time."""
        sim = JanusSim()
        
        trace = generate_llm_trace(context_length=1024, hidden_dim=4096)
        
        start = time.time()
        sim.run(trace)
        elapsed = time.time() - start
        
        # Should complete in under 5 seconds on modern hardware
        assert elapsed < 5.0, \
            f"Simulation runtime regression: {elapsed:.2f}s > 5.0s"


class TestBenchmarkCoverage:
    """Test benchmark suite covers important scenarios."""

    def test_covers_high_locality_workload(self):
        """Test benchmarks include high-locality workload."""
        sim = JanusSim()
        trace = generate_sequential_trace(start_addr=0x50000, num_accesses=100)
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # High locality should achieve good hit rate
        assert metrics.hit_rate > 80.0, "High locality workload should hit well"

    def test_covers_low_locality_workload(self):
        """Test benchmarks include low-locality workload."""
        from src.benchmarks.trace_generator import generate_random_trace
        
        sim = JanusSim()
        trace = generate_random_trace(num_accesses=100, addr_range=(0, 10_000_000), seed=42)
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Low locality should have poor hit rate
        assert metrics.hit_rate < 50.0, "Low locality workload should miss frequently"

    def test_covers_realistic_llm_workload(self):
        """Test benchmarks include realistic LLM workload."""
        sim = JanusSim()
        trace = generate_llm_trace(context_length=512, hidden_dim=2048)
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Realistic workload should have moderate to high hit rate
        assert metrics.hit_rate > 50.0, "LLM workload should have reasonable hit rate"


class TestBenchmarkUtilities:
    """Test benchmark utility functions."""

    def test_metrics_reporting(self):
        """Test metrics can be retrieved and reported."""
        sim = JanusSim()
        trace = [("READ", 0x60000)] * 10
        
        sim.run(trace)
        metrics = sim.get_metrics()
        
        # Should be able to access all metrics
        assert hasattr(metrics, 't1_hits')
        assert hasattr(metrics, 't1_misses')
        assert hasattr(metrics, 'total_cycles')
        assert hasattr(metrics, 'hit_rate')
        assert hasattr(metrics, 'p50_latency')
        assert hasattr(metrics, 'p90_latency')
        assert hasattr(metrics, 'p99_latency')

    def test_report_generation(self):
        """Test report generation doesn't crash."""
        sim = JanusSim()
        trace = generate_sequential_trace(start_addr=0x70000, num_accesses=50)
        
        sim.run(trace)
        
        # Should not raise exception
        try:
            sim.report()
            success = True
        except Exception:
            success = False
        
        assert success, "Report generation should not crash"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])